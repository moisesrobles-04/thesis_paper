\documentclass[chapters]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{caption}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{biblatex}

\graphicspath{ {./figures/} }
\addbibresource{reference.bib}

\begin{document}

\section{Literature Review}

\subsection{Large Language Models (LLM)}
Natural language processing (NLP) has always been an intricate field because of the complexity of how humans communicate.
It is challenging for a computer to replicate or learn text communication and expressions. However, this changed with the introduction
of Large Language Models (LLM) \textbf{[REFERENCE]}. These models are trained with large amounts of data to replicate human-like patterns,
and all was made possible because of transformers. This architecture differs from others because it uses self-attention \textbf{[REFERENCE]} \ref{transformer}.
This self-attention finds dependencies between all words in a text, short and long-term. In other words, it combines all possible 
positions of words within a text and assigns them a vector value. This enables the model to understand the meaning of words within a sentence. 

\begin{figure}[!htb]
    \centering
        \includegraphics[width=0.75\linewidth]{images/transformers_architecture.png}
        \caption{The transformer architecture}
        \label{transformer}
\end{figure}



The are three different architectures for Large Language Models, encoder-only, decoder-only, and encoder-decoder. Each one has advantages on specific tasks.

\subsubsection{Encoder-only models}
These models are like BERT.
This type of model predict by masking specific words in a sentence.
The are better for classification and sentiment analysis.

\subsubsection{Decoder-only models}
For these model we have the well known GPT-3, Mistral, and LLaMa.
They receive one input and try to predict the entire text.
They are good for summarizing and text-generation.


\subsubsection{Encoder-Decoder models}
The encoder-decoder models are T5.
They mask entire sequences of text.
Good for translation and question and answering. \cite{9906925}

\subsection{Health Misinformation}

\printbibliography %Prints bibliography

\end{document}