\section{Performance Evaluation}
\subsection{System Setup}
\subsubsection{Performance Metrics}
To evaluate the models effectiveness we used the following metrics:

\begin{itemize}
	\item{\textbf{Precision}}: To evaluate the proportion of positives examples that the model classified as positive that are actually positive. 
	\item{\textbf{Recall}}: To evaluate the proportion of the positives examples that the model classified correctly against all positives.
	\item{\textbf{F1}}: To balance the precision and recall scores.
	\item{\textbf{Elapsed Time}}: The time needed to complete the models finetuning stage.
	\item{\textbf{BERTScore}}: Compute a similarity score between two different sentences, to verify that the rebuttal generated by the LLM relates to the classified text \cite{zhang2020bertscoreevaluatingtextgeneration}.
\end{itemize}

\subsubsection{Hardware}
The node's specification can be found on Table \ref{table:hardware}.
\begin{table}[ht!]
\centering
\caption{Cluster's Node Specifications}
{\footnotesize
\begin{tabular}{||c | c||} 
 \hline
\textbf{Hardware} & \textbf{Description} \\ 
 \hline
 Hard Disk & 250GB  \\ 
 \hline
 RAM & 87.9GB  \\ 
 \hline
 Processor & Intel(R) Xeon(R) Silver 4214 @ 2.20GHz \\ 
 \hline
 GPU & NVIDIA TESLA V100s 32GB \\
 \hline
\end{tabular}
}
\label{table:hardware}
\end{table}

\subsubsection{Software}
There were various software tools used for the project. The training, ETL pipeline, and REST API were implemented with Python 3.9.19. To finetune the models we used PyTorch 2.0.1 with GPU support enabled for
CUDA 11.7. To initialize and use the models we relied on  the Transformers 4.34.0 library. The models were imported from Hugging Face (HF) \cite{huggingface}. We used BERT, T5, and LLaMa-2 for our research. To reduce the model memory usage, we use the PEFT 0.12.0 and bitsandbytes library for LoRA. In Table \ref{table:LLM} we see each model specification. Also, we use Postgres 14 to store our extracted research papers. In addition, we added Chroma 0.4.24 as our vector database. Next, for the RAG process, we installed Ollama \cite{ollama} and LangChain 0.1.16 to run LlaMa3.1 (8B) \cite{touvron2023llamaopenefficientfoundation} parameter model. Finally, to create the UI to show the results we used React.

  \begin{table}[ht!]
\centering
\caption{LLM Specifications}
{\scriptsize
\begin{tabular}{||c | c | c | c||} 
 \hline
\textbf{Model} & \textbf{HF name} & \textbf{Architecture} & \textbf{Parameters} \\ [0.5ex] 
 \hline
 BERT & bert-base-uncased & Encoder Only & 110M \\ 
 \hline
 T5 & t5-base & Encoder-Decoder & 220M \\
 \hline
 LlaMa-2 & Llama-2-7b-hf & Decoder-only & 7B \\
 \hline
\end{tabular}
}
\label{table:LLM}
\end{table}

\subsection{Fine-tuning}

The Large Language Models (LLM) used for this paper are pre-trained, meaning that the model learned how words relates to each other, the model is trained to understand a language. This process is computationally expensive and requires large amount of data. Instead of creating a model from scratch, we can teach an existing model to learn a new task such as text classification, translation, generation, or other. This is called fine-tuning, in this case was used to teach the model to classify two types of texts: health-related and misinformation-related. 
\newline
\newline
The health-related dataset consists of over 12,000 tweets extracted by the previous THS project. Said dataset was classified on three category: related, unrelated, and ambiguous tweets. A second dataset was created with data from different sources such as news, social medias, and blogs classified as misinformation and non-misinformation \textbf{\textit{[Insert References]}}. 

The models used for the classification process were Bert, T5, and LLaMa-2. 


\subsubsection{Training Parameters}
After validating that we can fine-tune the models with our hardware, we must select the hyperparameters for our training. We present the parameters used in Table \ref{table:hyperparameters}, and the description for each are as follows::

\begin{table}[!ht]
	\centering
	\caption{Fine-tuning Hyperparameters}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Parameter} & \textbf{value} \\ [1.5ex] 
		\hline
		Learning Rate & 5E-6  \\ [1ex]
		\hline
		Batch size & 16  \\[1ex]
		\hline
		Epochs & 20 \\[1ex]
		\hline
		Gradient Accumulation & 8 \\[1ex]
		\hline
		Weight\_decay & 0.1 \\[1ex]
		\hline
		Evaluation Step & 50 \\[1ex]
		\hline
		Evaluation Batch & 2 \\[1ex]
		\hline
		Evaluation Accumulation & 16 \\[1ex]
		\hline
		Warm-Up & 450 \\[1ex]
		\hline
		Metric & f1 \\[1ex]
		\hline
	\end{tabular}
	\label{table:hyperparameters}
\end{table}

\subsection{Fine-tuning Results}

\subsubsection{Health-Related Classification}
We present the results of the health classification process and compare them with the best overall model of the previous THS project \cite{8622504}. That work concluded
that their best model is an LSTM, with no attention, and a GRU layer.

\subsubsection{Precision}
\begin{table}[H]
	\centering
	\caption{Health Related Precision Result}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Model} & \textbf{Result} \\ [0.5ex] 
		\hline
		BERT & 0.85  \\
		\hline
		LLaMa-2 & 0.94 \\ 
		\hline
		LSTM GRU NO ATTENTION & 0.83  \\
		\hline
		T5 (Causal) & 0.85 \\
		\hline
		T5 (Sequence) & 0.48 \\
		\hline
	\end{tabular}
	\label{table:HealthPrecision}
\end{table}

Table \ref{table:HealthPrecision} shows the result for the precision metric for the related classification. For clarity, we focus on this class because our project
goal is to detect health-related misinformation. The best-performing model here was the LLaMa-2 model, which has a precision of 94\%. Tied for second place
are BERT and T5 (Causal), with 85\% precision. Next is the THS model, with 83\% precision. Lastly, we have a T5 (Sequence) model with a relatively low
precision of 48\%.

\subsubsection{Recall}
\begin{table}[H]
	\centering
	\caption{Health Related Recall Result}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Model} & \textbf{Result} \\ [0.5ex] 
		\hline
		BERT & 0.91  \\
		\hline
		LLaMa-2 & 0.84 \\ 
		\hline
		LSTM GRU NO ATTENTION & 0.89  \\
		\hline
		T5 (Causal) & 0.95 \\
		\hline
		T5 (Sequence) & 0.44 \\
		\hline
	\end{tabular}
	\label{table:HealthRecall}
\end{table}

Table \ref{table:HealthRecall} shows the result for the recall metric for the related classification. When comparing this metric with the THS investigation, there is a noticeable difference. Their results
show that only the LSTM layer, no attention, and a GRU layer model was the only one with a result over 80\%. In contrast, most of our models had a score of at least 80\%. Comparing this result
with the precision table (Table \ref{table:HealthPrecision}, our model did not score drastically lower. Here, our best model was T5 (Causal), with a performance of 95\% in recall. Next, we have
BERT with 91\%, followed by the THS model with 89\%. Then, the model with the highest precision, LLaMa-2, ended with 84\%. Lastly, we have again the T5 (Sequence) model with a 44\%,
lower than its precision.

\subsubsection{F1}
\begin{table}[H]
	\centering
	\caption{Health Related F1 Result}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Model} & \textbf{Result} \\ [0.5ex] 
		\hline
		BERT & 0.88  \\
		\hline
		LLaMa-2 & 0.89 \\ 
		\hline
		LSTM GRU NO ATTENTION & 0.86  \\
		\hline
		T5 (Causal) & 0.90 \\
		\hline
		T5 (Sequence) & 0.46 \\
		\hline
	\end{tabular}
	\label{table:HealthF1}
\end{table}

Table \ref{table:HealthF1} shows the result for the F1 metric for the related classification. The F1 score is the balance between precision and recall. When we compare the results with the
THS model, most models have a higher F1 score. The results show that T5 (Causal) had 90\%, LLaMa-2 with 89\%, BERT ended with 88\%, and T5 (Sequence) scored a 46\%. In contrast,
the THS model had an 86\%, ending in second to last place.

\subsubsection{Training Time}

In Figure \ref{fig:HealthTime}, we present our model's training time. As mentioned earlier, we trained each model for 20 epochs with a batch size of 16. BERT trained faster than
any other model, which took 3.12 hours. The T5 (Sequence) model took 24.03 hours, while the T5 (Causal) model finished in 26.15 hours. Lastly, LLaMa-2 took 85.43 hours, over three days, to
train. Based on these results, we can infer that models with fewer parameters train faster. BERT trained 27.4x faster than LLaMa-2.

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[scale=0.85]
    \begin{axis}[
        ybar,                        
        enlargelimits=0.15,        
        xlabel={Models},     
        ylabel={Training Time (hours)},       
        symbolic x coords={Bert, LlaMa-2, T5 (Causal), T5 (Sequence)}, 
        xtick=data,                  
        ticklabel style={font=\small},
        nodes near coords,    
        bar width=25pt,          
        ymin=0                       
    ]
        \addplot  [fill=black] coordinates {(Bert, 3.12) (LlaMa-2, 85.43) (T5 (Causal), 26.15) (T5 (Sequence), 24.03)};
    \end{axis}
\end{tikzpicture}
	\caption{Health-Related Models Training Time} %specify caption
	\label{fig:HealthTime}
\end{figure}


\subsection{Misinformation Classification}
In this section, we present the misinformation classification results. However, in this section, we did not compare with the THS model because they did not train a model to classify
misinformation.

\subsubsection{Precision}
\begin{table}[H]
	\centering
	\caption{Misinformation Precision Result}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Model} & \textbf{Result} \\ [0.5ex] 
		\hline
		BERT & 0.90  \\
		\hline
		LLaMa-2 & 0.98 \\ 
		\hline
		T5 (Causal) & 0.99 \\
		\hline
		T5 (Sequence) & 0.99 \\
		\hline
	\end{tabular}
	\label{table:MisinformationPrecision}
\end{table}

Table \ref{table:MisinformationPrecision} shows the result for the precision metric for the misinformation classification. In this case, we focus on the misinformation
class because it is our project goal. Our best-performing models here were both T5 models, which have a precision of 99\%. The next model with the highest precision
was LLaMa-2, with a score of 98\% precision. The lowest-performing model was BERT, which resulted in 90\% precision. Nonetheless, all models had a score of at least 90\%.


\subsubsection{Recall}
\begin{table}[H]
	\centering
	\caption{Misinformation Recall Result}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Model} & \textbf{Result} \\ [0.5ex] 
		\hline
		BERT & 0.94  \\
		\hline
		LLaMa-2 & 0.95 \\ 
		\hline
		T5 (Causal) & 0.92 \\
		\hline
		T5 (Sequence) & 0.85 \\
		\hline
	\end{tabular}
	\label{table:MisinformationRecall}
\end{table}

Table \ref{table:MisinformationRecall} shows the recall metric's results for the misinformation classification. When we compare these results with the precision table 
(Table \ref{table:MisinformationPrecision}), our model maintains a similar score. Here, the model with the best results was LLaMa-2, with a performance of 95\%.
Next, we have BERT with 94\% and T5 (Causal) with a 92\% recall. Finally, T5 (Sequence) ended with 85\% in the recall.


\subsubsection{F1}
\begin{table}[H]
	\centering
	\caption{Misinformation F1 Result}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Model} & \textbf{Result} \\ [0.5ex] 
		\hline
		BERT & 0.92  \\
		\hline
		LLaMa-2 & 0.97 \\ 
		\hline
		T5 (Causal) & 0.96 \\
		\hline
		T5 (Sequence) & 0.92 \\
		\hline
	\end{tabular}
	\label{table:MisinformationF1}
\end{table}


Table \ref{table:MisinformationF1}  shows the result for the F1 metric for the misinformation classification. This result balances precision and recall. The results show that the model
with the best F1 was LLaMa-2 with a 97\%. Next is T5 (Causal) with a 96\% F1, and tied to last, we have BERT and T5 (Sequence) with 92\%. All models had an optimal F1 score over 90\%.


\subsubsection{Training Time}

In Figure \ref{fig:MisinformationTime}, we present the training time for the misinformation models. We trained the models using the same parameters as in the Health-Related classification.
When we compare these results with the Health-Related (Figure \ref{fig:HealthTime}), the average training time was less. A possible reason is that we used less data for the training. The
only exception was BERT, which took 1.54 hours to train. Next is T5 (Causal) with 6.18 hours and T5 (Sequence) with 3.48 hours. Finally, LLaMa-2 took 47.33 hours to train. A
noticeable difference was T5 (Causal), which trained 4.23x faster for this dataset compared to the health-related dataset. Nonetheless, BERT finished the fastest, being 8.10x faster than LLaMa-2.

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[scale=0.85]
    \begin{axis}[
        ybar,                        
        enlargelimits=0.15,        
        xlabel={Models},     
        ylabel={Training Time (hours)},       
        symbolic x coords={Bert, LlaMa-2, T5 (Causal), T5 (Sequence)}, 
        xtick=data,                  
        ticklabel style={font=\small},
        nodes near coords,    
        bar width=25pt,          
        ymin=0                       
    ]
        \addplot  [fill=black] coordinates {(Bert, 1.54) (LlaMa-2, 47.33) (T5 (Causal), 6.18) (T5 (Sequence), 3.48)};
    \end{axis}
\end{tikzpicture}
	\caption{Misinformation Models Training Time} %specify caption
	\label{fig:MisinformationTime}
\end{figure}

\subsection{BERTScore}

Our model BERTScores'  F1 is shown in Table \ref{table:BERTScore}. We calculate these values by taking the average score of the 71 texts classified as health-related and misinformation. The table shows that both models, had a 82\% F1 score. That means that the generated output are closely related but not exactly the same. 

\begin{table}[H]
	\centering
	\caption{BERTScore F1 Results}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Model} & \textbf{Result} \\ [0.5ex] 
		\hline
		LLaMa-3.1 & 82\%  \\
		\hline
		GPT-3.5-turbo & 82\% \\ 		
		\hline
		\end{tabular}
	\label{table:BERTScore}
\end{table}



\section{Discussion}

The results present that most LLMs outperformed the THS model. We applied preprocessing techniques such as replacing links, mentions, and hashtags with special tokens. The training
parameters for all models were 20 epochs, batch size of 16, 8bit initialization, and adamw\_8bit optimizer. Additionally, the LoRA hyperparameters were 16 for the rank, an alpha of 32, a dropout
of 0.05, and a bias for all parameters. The score metrics we used for the model evaluation were precision, recall, and F1. In our case, we want to focus on F1. That metric helps us reduce false
negative results but not overclassify false positives.

Our model with the best result for the health-related classification (Table \ref{table:HealthF1}) was T5 (Causal), with a 90\% F1 score, while the THS model had 86\%. However, the trade-off
for this model is that the training is computationally expensive. The reason is that labels in T5 (Causal) are text instead of numbers. Those texts must be embedded, which requires more
processing power. Also, that model cannot have class weights because of the structure of the embedding. Additionally, the training time (Figure \ref{fig:HealthTime}) for T5 is more extensive
when compared to BERT, 8.4x. Now, BERT had a slightly lower result with 88\%. Nonetheless, when we factor in the training time and processing power, this model is more efficient.

The model with the highest F1 score for the misinformation classification (Table \ref{table:MisinformationF1}) was LLaMa-2, followed by T5 (Causal), 97\% and 96\%, respectively. These two models
have desirable results for this classification task. However, both require high computational power, and LLaMa-2 has an extensive training time (Figure \ref{fig:MisinformationTime}) compared to BERT,
30.90x more. It is important to notice that in both scenarios, LlaMa-2 and T5 (Causal) outperform the other models by a slight margin. These models contain a Decoder element, which can help them
generate and understand text appropriately.

For the BERTScore, we see that both models had an identical performance (Table \ref{table:BERTScore}). A possible reason is that the RAG process gives sufficient context to generate a coherent response. However, both models
have their trade-offs. To use GPT-3.5-turbo, we must pay OpenAI to request their API. On the other hand, LLaMa-3.1 ran with Ollama locally, and we need sufficient memory to execute the model. The
use case for each one depends on the user's hardware.

This project focuses on social media posts, and we know that there are frequent changes in how users interact. Additionally, when new diseases are found or named, we must retrain most
models to add these words to their vocabulary. Retraining can be costly if the model has many parameters and requires extensive training. Thus, we can say that BERT had overall results
that can help combat health misinformation on social media. That model had an F1 score of 88\% in health and 92\% in misinformation classification. Finally, that model took less overall time
to train and used the least amount of RAM.




