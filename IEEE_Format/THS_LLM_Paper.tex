\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{sidecap}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{supertabular}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage[margin=3cm]{geometry}
\usepackage{setspace}
\usepackage{url}
\usepackage{indentfirst}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{pdflscape}
\usepackage{lipsum}
\usepackage{blindtext}
\usepackage{pgfplots}
\usepackage{fancyhdr}
\usepackage{pgfplotstable}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{enumitem} % For list customization
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tocloft}
\usepackage{tcolorbox}
\usepackage{notoccite} % PREVENTS CITES IN CAPTIONS FROM MISNUMBERING YOUR REFERENCES https://tex.stackexchange.com/questions/302594/citation-inside-a-caption-dont-follow-order-of-appearance
\pgfplotsset{compat=1.7}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18} % Use a recent version for compatibility
\usepackage[numbers,sort&compress]{natbib}
%\renewcommand{\bibname}{Bibliography} % or other title eg. Bibliography
\bibliographystyle{ieeetr} %styles: abbrv, acm, alpha, apalike, ieeetr, plain, siam, unsrt.
\usepackage{subfiles} % Best loaded last in the preamble
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    

\title{\huge FIGHTING HEALTH-RELATED MISINFORMATION IN SOCIAL MEDIA WITH LARGE LANGUAGE MODELS}

\author{
	\IEEEauthorblockN{Moisés Robles Pagán}
	\IEEEauthorblockA{
		\textit{Electrical and Computer Engineering Department} \\
		\textit{University of Puerto Rico - Mayagüez}\\
		moises.robles@upr.edu
	}
	\and 
	\IEEEauthorblockN{Manuel Rodríguez Martínez}
	\IEEEauthorblockA{
		\textit{Electrical and Computer Engineering Department}\\
		\textit{University of Puerto Rico - Mayagüez}\\
		manuel.rodriguez7@upr.edu
	}
}
\begin{document}

\maketitle

\begin{abstract}
Combating disinformation in social media is a critical problem, notably when the disinformation targets healthcare. We explore how to fine-tune Large Language Models (LLM) to counteract health-related disinformation on social media. The fine-tuned base models for this project are T5, BERT, and LlaMa-2. We divide the fine-tuning into two sections: 1) classifying if the text is health-related and 2) verifying if the text contains disinformation. To rebut disinformation we use Retrieval Augmented Generation (RAG) to query trusted medical sources. Our experiment shows that the models can classify health-related with 94\% precision, 95\% recall, and 90\% F1. We also show that we classify disinformation texts with 99\% precision, 95\% recall, and 97\% F1. We present an investigation that can help health experts combat and rebut disinformation on different social media platforms.
\end{abstract}

\begin{IEEEkeywords}
Large Language Model, Misinformation, Transformers, Vector Databases
\end{IEEEkeywords}

\subfile{chapters/Chap1_Introduction}

\subfile{chapters/Chap2_LiteratureReview}

\subfile{chapters/Chap3_Architecture}

\subfile{chapters/Chap4_PerformanceEvaluation}

\subfile{chapters/Chap5_Conclusions}

\bibliography{reference} 

%IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}




