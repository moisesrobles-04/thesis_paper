



%__________   ABSTRACT ENGLISH ________________________________
\vspace*{0.5in}
\begin{center}
\section*{ABSTRACT}
\end{center}
\addcontentsline{toc}{section}{ABSTRACT} %para que aparezca en la tabla de contenido

\noindent
Combating disinformation in social media is a critical problem, notably when the disinformation targets healthcare. We explore how to fine-tune Large Language Models (LLM) to counteract health-related disinformation on social media. The fine-tuned base models for this project are T5, BERT, and LlaMa-2. We divide the fine-tuning into two sections: 1) classifying if the text is health-related and 2) verifying if the text contains disinformation. To rebut disinformation we use Retrieval Augmented Generation (RAG) to query trusted medical sources. Our experiment shows that the models can classify health-related with 94\% precision, 95\% recall, and 90\% F1. We also show that we classify disinformation texts with 99\% precision, 95\% recall, and 97\% F1. We present an investigation that can help health experts combat and rebut disinformation on different social media platforms.

%____________________________________________________________





\newpage




%__________   ABSTRACT ESPANOL  ______________________________

\vspace*{0.5in}
\begin{center}
\section*{RESUMEN}
\end{center}
\addcontentsline{toc}{section}{RESUMEN} %para que aparezca en la tabla de contenido

\noindent
Combatir la desinformaci\'on en las redes sociales es un problema cr\'itico, notablemente cuando se trata de desinformaci\'on en el campo de la salud. Aqu\'i exploramos como afinamos un Large Language Model (LLM) para contrarrestar desinformaci\'on relacionada a salud en las redes sociales. Los modelos bases a entrenar en esta investigaci\'on son T5, BERT y LLaMa-2. Dividimos este proceso de entrenar en 2 secciones: 1) clasificar si un texto es relacionado a salud y 2) verificar si el texto contiene desinformaci\'on. Para refutar la desinformaci\'on utilizamos Retrieval Augmented Generation (RAG) para hacer consultas a fuentes m\'edicos confiables. Nuestro experimento muestra que los modelos pueden clasificar textos relacionados a salud con una precisi\'on de 99\%, recall de 95\% y F1 de 97\%. Adem\'as, presentamos que en los resultados de clasificaci\'on de desinformaci\'on se obtuvo un 99\% de precisi\'on, recall de 95\% y F1 de 97\%. Presentamos una investigaci\'on que puede asistir a los expertos en salud a combatir y refutar desinformaci\'on en las distintas redes sociales.
%____________________________________________________________