
\chapter{Methodology}  

\section{Performance Metrics}
To evaluate the models effectiveness we used the following metrics:

\begin{itemize}
	\item{\textbf{Precision}}: This metric evaluates the proportion of positive that the model classified as positive that are actually positive.
	\item{\textbf{Recall}}: This metric evaluates the proportion of the positives that the model classified correctly again all positives.
	\item{\textbf{F1}}: This metric balances the precision and recall scores.
	\item{\textbf{Elapsed Time}}: To evaluate the models training time.
	
\end{itemize}


\section{Hardware}
This experiment was implemented in a cluster. Only one node was used for the finetuning and to host the application. The hardware specification
for the node is found on Table \ref{table:hardware}.

\begin{table}[ht!]
\centering
\caption{Cluster's Node Specifications}
\begin{tabular}{||c | c||} 
 \hline
\textbf{Hardware} & \textbf{Description} \\ [0.5ex] 
 \hline
 Operating System & Ubuntu 20.04.2 LTS \\ [0.5ex] 
 \hline
 Hard Disk & 250GB  \\ [0.5ex] 
 \hline
 RAM & 87.9GB  \\ [0.5ex] 
 \hline
 Processor & Intel(R) Xeon(R) Silver 4214 @ 2.20GHz \\ [0.5ex] 
 \hline
 GPU & NVIDIA TESLA V100s 32GB \\ [0.5ex] 
 \hline
\end{tabular}
\label{table:hardware}
\end{table}


\section{Software}
There were various software tools used for the project. The training, ETL pipeline,
and REST API were implemented with Python 3.9.19. To finetune the models we used PyTorch 2.0.1 with GPU support enabled for
NVIDIA CUDA 11.7. To initialize and use the models we implemented the Transformers 4.34.0 library. The models were imported from
Hugging Face (HF) \textbf{[REFERENCE]}. We used BERT, T5, and LLaMa-2 for our research. To reduce the model memory usage, we use
the PEFT 0.12.0 and bitsandbytes library for LoRA. In Table \ref{table:LLM} we see the specifications for each model. All these models are base
model, which means they are not train to accomplish a specific task. Also, we use Postgres 14 to store our extracted research papers. In addition, we
implemented Chroma 0.4.24 as our vector database. Finally, for the RAG process, we installed Ollama tu run LlaMa3.1 8B parameter model. 


  \begin{table}[ht!]
\centering
\caption{LLM Specifications}
\begin{tabular}{||c | c | c | c||} 
 \hline
\textbf{Model} & \textbf{HF name} & \textbf{Architecture} & \textbf{Parameters} \\ [0.5ex] 
 \hline
 BERT & bert-base-uncased & Encoder Only & 110M \\ 
 \hline
 T5 & t5-base & Encoder-Decoder & 220M \\
 \hline
 LlaMa-2 & Llama-2-7b-hf & Decoder-only & 7B \\
 \hline
\end{tabular}
\label{table:LLM}
\end{table}
 


\section{Datasets}

This section describes the health-related and misinformation datasets used for training, their structure, and the preprocessing techniques applied.

\begin{table}[H]
	\centering
	\caption{Training Datasets}
	\begin{tabular}{||c | c | c| c||} 
		\hline
		\textbf{Dataset} &
		\textbf{Category} & \textbf{Label}& \textbf{\# of records} \\ [1.5ex] 
		\hline
		\multirow{3}{6.75em}{Health Related Dataset} & Unrelated & 0 & 3828  \\[1ex]
		& Related & 1 & 7848  \\ [1ex]
		& Ambiguous & 2 & 765 \\[1ex]
		\hline
		\multirow{2}{6.75em}{Misinformation Dataset} & Misinformation & 0 & 3638\\ [1ex]
		& Not Misinformation & 1 & 5111  \\[1ex]
		\hline
	\end{tabular}
	\label{table:dataset}
\end{table}


\subsection{Health-related Dataset}
The health-related dataset comprises over 12,441 tweets extracted from the previous THS project.  Health professionals labeled the tweets in this dataset as related, unrelated, or ambiguous. As shown in Table \ref{table:dataset} this dataset is imbalanced; thus, we applied class weights to the loss function. The imbalance can cause the model to predict the most frequent class disproportionately, reducing its ability to generalize.These weights assign higher penalties to errors in underrepresented classes. The weights prevent the models from overfitting, mitigating the effects of class imbalance during training. The weight penalty for each class is shown in Table \ref{table:healthweight}.

\begin{table}[H]
	\centering
	\caption{Health-Related Dataset Weight Penalty}
	\begin{tabular}{||c | c||} 
			\hline
			\textbf{Category} & \textbf{Weight Penalty} \\ [1.5ex] 
			\hline
			Unrelated & 3.25 \\ [1ex]
			\hline
			Related & 1.58 \\[1ex]
			\hline
			Ambiguous & 16.26 \\[1ex]
			\hline
		\end{tabular}
	\label{table:healthweight}
\end{table}


\subsection{Misinformation Dataset}
The misinformation dataset, with 8,749 texts, combines data from different sources such as news, social media, and blogs. These records have two labels: misinformation and non-misinformation. As in the health-related dataset, we also applied class weights because of the imbalance, as Table \ref{table:dataset} shows. The weight penalties used for the imbalance is shown in Table \ref{table:misinformationweight}. Multiple datasets from different sources were combined for this experiment. The dataset in \cite{stephencrone2022} consists of social media texts that mention the monkeypox. We only used "text" and "binary\_class" columns from that dataset. Another source was \cite{coviddata}, which classified Covid-19 texts. That dataset contains articles and posts from different online sources. They labeled the data based on official sources that validated the text claims. In this dataset, we only used the "title" column and the label assigned to the file. We used that dataset because, occasionally, online users share news reports or articles based on the headline alone. Next, the last dataset \cite{covidunesco} data, comes from a variety of sources. The dataset only contains misinformation records such as posts, news, videos, emails, messages, and others. We selected English-only records from that dataset because BERT is limited to this language. Now, we used only the "Title", "Narrative\_Description" columns. These two columns were appended as one string for the model to train.


\begin{table}[H]
	\centering
	\caption{Misinformation Dataset Weight Penalty}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Category} & \textbf{Weight Penalty} \\ [1.5ex] 
		\hline
		Misinformation & 2.41  \\ [1ex]
		\hline
		Not Misinfomation & 1.71  \\[1ex]
		\hline
	\end{tabular}
	\label{table:misinformationweight}
\end{table}


\subsection{Data Preprocessing}
Before we start fine-tuning the models, we preprocessed both datasets. As initially mentioned, we want as much context as possible for our model to train. That meant the classification
process would include links, mentions, and hashtags. These elements are important for context because users can convey sentiments that could be relevant to text. However, these elements contain various characters, and the embedding might struggle with out-of-vocabulary or irregular
patterns. Thus, we opted to use special tokens to replace them. We use special tokens to replace elements with patterns that do not contribute to semantic meaning, such as random
strings in URLs. For example, some URLs are strings with random values, which can result in the embedding assigning the wrong values. Because of this, we replace these items with
their respective tokens. Any URL was replaced by \textit{[LINK]}, mentions by \textit{[MENTION]}, and hashtags by \textit{[HASHTAG]}. That allows us to keep the elements and give them some importance in the text. An example of this is shown below.

\noindent \textbf{Example 1:}
 \begin{tcolorbox}[colback=gray!10, colframe=black!70, title=Original Text]
	\texttt{%
listening to these amazing infectious disease experts talk about \#influenza at the @nmnh/@asmicrobiology \#flu program like https://t.co/ehnpz6nsyg
	}
\end{tcolorbox}

\begin{tcolorbox}[colback=white, colframe=black!70, title=Preprocessed Result]
	\textit{%
listening to these amazing infectious disease experts talk about [HASHTAG] at the [MENTION]/[MENTION] [HASTAG] program like [LINK]
	}
\end{tcolorbox}


\section{Fine-tuning}

The LLMs used for this paper are pre-trained, meaning that the model learned how words relate to each other. In other words, the model is trained to understand a language.
This process is computationally expensive and requires a large amount of data. Instead of creating a model from scratch, we can teach an existing model to learn a new task such as text classification, translation, generation, or other. That process is called fine-tuning. Fine-tuning is when an existing model is trained to accomplish a task. In this case, we taught the models to classify two types of texts: health-related and misinformation-related texts. 


We identified three models to perform these classification tasks. The models used for this experiment are Bert, T5, and LLaMa-2. Each one has a different architecture, as shown in Table \ref{table:LLM}. As mentioned, these models are their base form. Additionally, these models have large numbers of parameters. That can be a drawback when fine-tuning because it requires excessive resources. 


\subsection{Memory Usage Estimation}
Our initial approach for the experiment was to use the model with default parameters. That meant using the base model with Adam optimizer and with a floating point of 32 (fp32). However, that was not possible because the model requires an excessive amount of memory. To estimate the resources that each model requires, we use the formula in \cite{transformer-math-eleutherai}. Their formula provides a result with an overhead of around 20\%.


\[ Memory_{model}, fp32 = (4 bytes/param) (No. param) \label{eq:model} \tag{1} \] 

For the example, we use T5, which has 220M parameters. If we use Equation \ref{eq:model}, T5 requires 839MB to initialize.


\[ Memory_{optimizer} = (12 bytes/param) (No. param) \label{eq:optimizer} \tag{2} \] 

Then, we calculate the optimizer state. We consider AdamW because it is the most common optimizer. When using Equation \ref{eq:optimizer} we get that the optimizer for T5 requires 2.46GB.


\[ Memory_{gradients} = (4 bytes/param) (No. param) \label{eq:gradients} \tag{3} \] 

Next, we have the gradient. The gradient memory usage is similar to the model. Equation \ref{eq:gradients} shows that it requires 839MB for T5.


\[ Memory_{activation} = sbhL(10 + \frac{24}{t} + 5 \frac{a \cdot s}{h \cdot t} ) \label{eq:activation} \tag{4} \] 

The final attributes that we need to consider are activation and batch size. We estimate the variables for Equation \ref{eq:activation} based on the experiments conducted. The first element is \textbf{s}, which refers to the token length sequence. Then, the \textbf{b} is for the batch size. The \textbf{h} stands for the model's hidden size and dimensionality. Now, \textbf{L} is the number of layers in the model. Next, we have \textbf{a} for the attention head of the model. Lastly, we have \textbf{t} for parallelism. The variables for each model can be found in Table \ref{table:ModelActivation}. Finally, if we calculate T5 activation memory usage, it results in 3.80GB.

\begin{table}[H]
	\centering
	\caption{Models Activation Parameters}
	\begin{tabular}{||p{1.75cm} | p{2cm} | p{1.75cm} | p{1.75cm} | p{1.5cm} | p{2cm} | p{2.75cm}||} 
		\hline
		\textbf{Model} & \textbf{Sequence (s)} & \textbf{Batch Size (b)} & \textbf{Hidden Size (h)} & \textbf{Layers (L)} & \textbf{Attention Head (a)} & \textbf{Parallelism (t)} \\ [1ex] 
		\hline
		BERT & 256 & 16 & 768 & 12 & 12 & 1 \\ [1ex]
		\hline
		T5 & 256 & 16 & 768 & 12 & 12 & 1  \\[1ex]
		\hline
		LLaMa-2 & 256 & 16 & 4096 & 2 & 32 & 1  \\[1ex]
		\hline
	\end{tabular}
	\label{table:ModelActivation}
\end{table}


With all these values calculated, we can estimate the memory usage per model using Equation \ref{eq:total}. The results for each model are presented in Table \ref{table:MemoryUsage}. These estimates show the minimum memory required for each model. For BERT we need 3.95GB, T5 is 6.00GB, and LLaMa-2 is 152.39GB. Knowing that our cluster only has a GPU with 32GB, it would not be possible to fine-tune LLaMa-2. However, there is an alternative that reduces the model size.
	
\[ Total Memory = Model + Optimizer + Gradients + Activation \label{eq:total} \tag{5} \] 

\begin{table}[H]
	\centering
	\caption{Models Memory Usage Estimate (MB)}
	\begin{tabular}{||c | c | c | c | c | c | c||} 
		\hline
		\textbf{Name} & \textbf{Parameters} & \textbf{Model} & \textbf{Optimizer} & \textbf{Gradient} & \textbf{Activation} & \textbf{Total} \\ [1ex] 
		\hline
		BERT & 110M & 419.62 & 1258.85 & 419.62 & 1944.00 & \textbf{4042.09} \\ [1ex]
		\hline
		T5 & 220M & 839.23 & 2517.70 & 839.23 & 1944.00 & \textbf{6140.16}  \\[1ex]
		\hline
		LLaMa-2 & 7B & 26702.88 & 80108.64 & 26702.88 & 11264.00 & \textbf{156042.40}  \\[1ex]
		\hline
	\end{tabular}
	\label{table:MemoryUsage}
\end{table}

\subsection{Parameter-Efficient Fine-Tuning (PEFT)}
PEFT is a library used to reduce the parameters of a model that will be fine-tuned. The library allows us to use the Low-Rank Adapter (LoRA). That adapter helps us reduce the model's memory usage. The main reason is that we can initialize the model in 8 bits instead of 32 bits. Also, because we have less precision, we can change the optimizer to adamw\_8bits. Additionally, we do not fine-tune the entire model, only modify a small percentage.

To determine how many parameters will be modified, we need to understand the hyperparameters of the LoRA configurations. Based on the research \cite{hu2021loralowrankadaptationlarge}, we can estimate the number of trainable parameters. Equation \ref{eq:TrainParam} can give the approximate trainable parameters. The variable \textbf{d\textsubscript{model}} is the hidden layer of the model. Then, \textbf{r} is the low-rank factor, which is how many parameters we train. The last attribute is the \textbf{L\textsubscript{weight}}, the number of weight matrices. That value is the product of the number of layers by the projection matrices. In our experiment, we use two model matrixes, the query matrix (Q) and the value matrix (V). The first represents what the query token is looking for in other tokens. The value matrix contains the information associated with each token; for more details about these matrixes, refer to \cite{vaswani2023attentionneed}.

\[ Trainable \ Parameter = 2 \cdot d_{model} \cdot r \cdot L_{weight} \label{eq:TrainParam} \tag{6} \] 

Currently, we can estimate the LoRA layer for each model. The hyperparameters and total trainable parameters of our experiment can be found in Table \ref{table:LoRA}.

\begin{table}[H]
	\centering
	\caption{LoRA Hyperparameters \& Total Trainable Parameters}
	\begin{tabular}{||c | c | c | c | c||} 
		\hline
		\textbf{Name} & \textbf{d\textsubscript{model}} & \textbf{r} & \textbf{L\textsubscript{weight}} & \textbf{Total Parameters} \\ [1ex] 
		\hline
		BERT & 768 & 16 & 24 & \textbf{589,824} \\ [1ex]
		\hline
		T5 & 768 & 16 & 24 & \textbf{589,824}  \\ [1ex]
		\hline
		LLaMa-2 & 4096 & 16 & 64 & \textbf{8,388,608}  \\ [1ex]
		\hline
	\end{tabular}
	\label{table:LoRA}
\end{table}

Now, we can estimate the new memory usage that each model will require. To ensure that we can use all models, we made some changes to the parameters. First, we reuse the batch size and token sequence from \ref{table:ModelActivation}. Second, we have adamw\_8bits as our optimizer. Finally, we will use the alternative equations from \cite{transformer-math-eleutherai} for the estimate. This time, LLaMa-2 will be the model for our calculation.

\[ Memory_{model}, int8 = (1 bytes/param) (No. param) \label{eq:peftmodel} \tag{7} \] 

For Equation \ref{eq:peftmodel}, we use the total number of parameters. We do this because we need to initialize the entire model even if we train just a few parameters.


\[ Memory_{optimizer} = (6 bytes/param) (No. param) \label{eq:peftoptimizer} \tag{8} \] 

For the optimizer, we use 8bit\_AdamW because it retains the advantages of AdamW but leaves a smaller memory footprint. In this case, we only consider the number of parameters to be trained. With Equation \ref{eq:peftoptimizer} and Table \ref{table:LoRA}, we calculate that the optimizer for LLaMa-2 requires 48MB.


\[ Memory_{gradients} = (1 bytes/param) (No. param) \label{eq:peftgradients} \tag{9} \] 

Next, we have the gradient. The gradient consumes a similar amount of memory as the model. Equation \ref{eq:peftgradients} shows that LlaMa-2 requires 6675MB.


The last element is the activation, and we reuse Equation \ref{eq:activation}. The variables that the equation requires are in Table \ref{table:ModelActivation}. Moreover, if we calculate LLaMa-2 activation memory usage, it results in 11GB. The new memory usage for each model is in Table \ref{table:PeftMemoryUsage}. That table shows that BERT requires 1.06GB, T5 is 1.16GB, and LLaMa-2 is 17.58 GB. Those results show that it is possible to train the models on our hardware without limitations. However, these estimates can vary depending on the tasks or the model's initialization method. Causal Language Modeling can require more resources than the estimate because we turn the labels' data into a token sequence. Other factors, such as dropout, the weight we give to each LoRA parameter, and the weight decay, also affect this result. Nonetheless, this approximation shows that it is possible to fine-tune the models.

\begin{table}[H]
	\centering
	\caption{PEFT Model Memory Usage Estimate (MB)}
	\begin{tabular}{||c | c | c | c | c | c | c||} 
		\hline
		\textbf{Name} & \textbf{PEFT Param.} & \textbf{Model} & \textbf{Optimizer} & \textbf{Gradient} & \textbf{Activation} & \textbf{Total} \\ [1ex] 
		\hline
		BERT & 589,824 & 104.90 & 3.38 & 1.13 & 972.00 & \textbf{1081.40} \\ [1ex]
		\hline
		T5 & 589,824 & 209.81 & 3.38 & 1.13 & 972.00 & \textbf{1186.31}  \\[1ex]
		\hline
		LLaMa-2 & 8,388,608 & 6675.72 & 48.00 & 16.00 & 11264.00 & \textbf{18003.72}  \\[1ex]
		\hline
	\end{tabular}
	\label{table:PeftMemoryUsage}
\end{table}


\subsection{Training Parameters}
After validating that we can fine-tune the models with our hardware, we must select the hyperparameters for our training. We present the parameters used in Table \ref{table:hyperparameters}, and the description for each are as follows::

\begin{table}[!ht]
	\centering
	\caption{Fine-tuning Hyperparameters}
	\begin{tabular}{||c | c||} 
		\hline
		\textbf{Parameter} & \textbf{value} \\ [1.5ex] 
		\hline
		Learning Rate & 5E-6  \\ [1ex]
		\hline
		Batch size & 16  \\[1ex]
		\hline
		Epochs & 20 \\[1ex]
		\hline
		Gradient Accumulation & 8 \\[1ex]
		\hline
		Weight\_decay & 0.1 \\[1ex]
		\hline
		Evaluation Step & 50 \\[1ex]
		\hline
		Evaluation Batch & 2 \\[1ex]
		\hline
		Evaluation Accumulation & 16 \\[1ex]
		\hline
		Warm-Up & 450 \\[1ex]
		\hline
		Metric & f1 \\[1ex]
		\hline
	\end{tabular}
	\label{table:hyperparameters}
\end{table}


\begin{itemize}
\item{\textbf{Learning Rate}}: Step size that the model takes to adjust its parameters.
\item{\textbf{Batch size}}: Number of training samples used in one iteration or step.
\item{\textbf{Epochs}}: Number of times the model iterates the entire training data.
\item{\textbf{Gradient Accumulation}}: Accumulation of multiple batches results during training to perform a weight update; this reduces memory usage.
\item{\textbf{Weight\_decay}}: A regularization method used to help the model generalize.
\item{\textbf{Evaluation Step}}: Number of update steps between two evaluations.
\item{\textbf{Evaluation Batch}}: Number of samples used in one iteration or step during evaluation.
\item{\textbf{Evaluation Accumulation}}: The accumulation of multiple batches results during evaluation to perform a weight update; this reduces memory usage.
\item{\textbf{Warm-Up}}: Number of warm-up steps for the learning rate.
\item{\textbf{Metric}}: The metric used to select the best model during the evaluation process.

\end{itemize}

Apart from these parameters, we split the data for each classification process as follows:

\begin{description}
\item{\textbf{Training dataset (70\%)}}: the data used to adjust the models' parameters during the fine-tuning training process.
\item{\textbf{Evaluation dataset (15\%)}}: the data used to validate the models' performance during the fine-tuning evaluation process. When each evaluation step finishes, we save the configuration for the model with the highest metric result. 
\item{\textbf{Test dataset (15\%)}}: we use this data to select the best model between a collection of trained and validated models.

\end{description}

After finishing the data preprocessing and configuring the model we start the training process. 


%\subsection{Health-Related Classification}
%
%We start the classification process with the health-related dataset. As shown in Table \ref{table:dataset} the dataset is imbalanced. That issue can result in the model training incorrectly because the model 
% The class weights values for this dataset is shown in Table \ref{table:healthweight}
%
%
%
%\subsection{Misinformation Classification}
%
%
%2. Misinformation classification
%
%    1. Sequence classification: 1, 2, or 3; 1 or 2.
%
%    2. Classification with text generation: Related, Unrelated, or Ambiguous; Misinformation or Not Misinformation.
%
%- Weighted average added for the sequence classification.

