

\chapter{Introduction}

\section{Motivation}

\noindent Nowadays, technology has advanced to the point that anyone can find any information in just a few seconds. Social media has been an essential
element in the search for information. The issue with this is that anyone can find, search, share, and even write anything, accurate or not. However, this dilemma
has caused problems in this modern era. If anyone can share anything, how can you be sure what is true? Users are susceptible to disinformation or misinformation.
In this context, misinformation refers to messages with false information dispersed because the author misunderstood facts. In contrast, disinformation refers to
messages with false information that are intentionally dispersed. The author of these messages has the intention of forming opinions based on false data. In either
case, false information spreads to readers as facts. Most social media platforms recommend that readers read from experts or official news outlets. Nevertheless,
the overwhelming amount of data makes it complicated to keep up with everything.

Currently, social media such as X (formerly known as Twitter) have "Community Notes" which clarify tweets that are misleading or misinforming. However, this system
depends totally on human interaction and is a slow and intricate process. On most occasions, when a "Community Note" is added to a tweet, the disinformation has
already been spread. The issue of detecting and preventing the spread of misinformation has not been an easy task, especially in the health field. Another important
factor mentioned in \cite{article} is that health-related textual misinformation is more engaging when compared with pictorial.

In recent times, it has been challenging for health officials to achieve the prevention of endemic or pandemics. Most of the time, these officials tend to make educational
campaigns for the population. However, social media misinformation can reduce the effectiveness of these campaigns. In addition, this is harder to counteract because
these can spread for longer times and reach different users \cite{article}. Some problems these experts have faced in the past years were misinformation
about vaccines, users invalidating safe measurements, and other issues.

The Twitter Health Surveillance (THS) system was designed to detect tweets related to health conditions \cite{8622504}. THS is a prototype system we are building at the University
of Puerto Rico, Mayag\"uez (UPRM). The project is designed as an integrated platform to help health officials collect tweets, determine if they are related to a medical condition, extract
metadata from them, and create a warehouse that can be used to analyze the data further. The THS Artificial Intelligence (AI) components used Long-Short Term Memory (LSTM)
and Gated Recurrent Unit (GRU) to classify tweets as being medically related, unrelated, or ambiguous. The data they used was from the Twitter API system, then processed
through the Hadoop ecosystem and stored in a Hive database.

Searching for training data about this topic is not an easy task. A problem with social media text is the informality, slang terms, or special characters. However, we use the
data from the THS project as our primary source for the health classification dataset. On the other hand, the misinformation dataset contains social media posts, articles,
websites, and others. Instead of using the THS architectures, we opted for Large Language Models (LLM). 

An LLM is a computational model with high capability in Natural Language Processing (NLP) thanks to its ability to make statistical relationships within texts. These models
have made breakthroughs in how computers interpret human language. Additionally, we can train them to accomplish text-based tasks. Some examples are classifications,
answering questions, making inferences, summarizing, and others. Thus, we can train them to classify and make inferences from a text.

We employ a transformer-based LLM in this project. These are effective in the natural language process field (NLP) \cite{DBLP:journals/corr/abs-2105-00813}.
The system detects health misinformation and provides context for its classification. Also, we did not preprocess the tweets because they could lose the context of the actual meaning if we
remove hashtags, mentions, and emojis. This system has a high chance of detecting misinformation and rebutting it to educate users. We built the prototype using Python, PyTorch, Chroma,
Ollama, and other open-source tools. In determining if a text is health-related, our system achieved a 90\% F1 score and a 97\% F1 if it contained misinformation. Additionally, our preliminary
results show that using official health sources with RAG helps the LLM rebut correctly. Hence, our system proved that it is possible to classify and rebut health-related misinformation.

\section{Objectives}

\noindent
The objectives of this project are as follows:
\begin{itemize}
	\item Identify and extract information from official health sources: This data will be stored in a vector database that the model will use as context to rebut the misinformation. The model will cite official health sources related to the tweets to sustain their classification.
	\item Identify and finetune a Large Language Model: Select an appropriate base Large Language Model architecture that will:
	\begin{enumerate}
		\item Detect if a text is health related.
		\item Determine if a text is misinformation.
		\item Use official health sources texts to combat the texts classified as misinformation and cite from the gather data.
	\end{enumerate}
	\item Compare with the previous version of THS: To measure the effectiveness of the classification with the LLM, we are going to compare it with the previous THS results and validate the advantages of a Large Language Model on solving Natural Language Processing problems. 
\end{itemize}

\section{Contributions}
\noindent
\begin{itemize}
	\item \textbf{Finetune Large Language Models for health classification on social media:} Large Language Models are being used for different fields nowadays. However, these do not focus on health misinformation on social medias. We present Large Language Models as a solution
	to classify and rebut health misinformation texts on social medias, and use research papers extracted from PubMed as context for the LLM.
	\item \textbf{Pending Contribution Title:} We used 12,441 texts for the health-related classification labeled as related, unrelated, or ambiguous. For the misinformation-classification we had 8,772 texts labeled as misinformation or not misinformation. For the model rebuttal, we extracted
	56,365 papers from PubMed.
	\item \textbf{Present a novel solution to misinformation rebuttal:} For misinformation rebuttal is necessary to have an understanding of what needs to be fact checked. Also, it is important to have the necessary context to make the correction. We extracted research papers that were added into a
	vector database. The database helped find similar chunks of texts to use as context for the misinformation rebuttal. 
	
\end{itemize}

\section{Outline}
\noindent
This paper has the following organization. Chapter 2 contains the literature review on Transformers, Large Language Models,
the different use cases of these models for classification, misinformation on social media, and the THS project. Additionally, we describe the
importance of disinformation on social medias. For Chapter 3, we can observe the problem description and methodology.
Later, on Chapter 4, we have our system architecture, and the projects pipeline for the training and classification. Chapter 5 presents
our results based on accuracy and performance. In Chapter 6, related works are presented, with our conclusion and suggestions 
or future work.