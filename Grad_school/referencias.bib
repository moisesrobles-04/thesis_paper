









% ____EXAMPLES OF REFERENCE TYPES: Journals, books, webpages, chapters____


@article{radiation2,
author = {L\'opez, A.M. and Soderstrom, K.G.},
title = {Insolation in {P}uerto {R}ico},
journal = {Journal of Solar Energy Engineering},
year = {1983},
volume = {},
number = {},
pages = {},
}

@article{einstein,
    author = "Albert Einstein",
    title = "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
    [{On} the electrodynamics of moving bodies]",
    journal = "Annalen der Physik",
    volume = "322",
    number = "10",
    pages = "891--921",
    year = "1905",
    DOI = "http://dx.doi.org/10.1002/andp.19053221004",
    keywords = "physics"
}

@book{dirac,
    title = {The Principles of Quantum Mechanics},
    author = {Paul Adrien Maurice Dirac},
    isbn = {9780198520115},
    series = {International series of monographs on physics},
    year = {1981},
    publisher = {Clarendon Press},
    keywords = {physics}
}

%for webpage references
@misc{knuthwebsite,
    author = "Donald Knuth",
    title = "Knuth: Computers and Typesetting",
    howpublished  = "http://www-cs-faculty.stanford.edu/~uno/abcde.html",
    note = "(accessed: 01.09.2016)",
    keywords = "latex,knuth"
}

@inbook{knuth-fa,
    author = "Donald E. Knuth",
    title = "Fundamental Algorithms",
    publisher = "Addison-Wesley",
    year = "1973",
    chapter = "1.2",
    keywords  = "knuth,programming"
}


% ____Official references________


@INPROCEEDINGS{8622504,
  author={Garzón-Alfonso, Cristian C. and Rodríguez-Martínez, Manuel},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={Twitter Health Surveillance (THS) System}, 
  year={2018},
  volume={},
  number={},
  pages={1647-1654},
  keywords={Twitter;Big Data;Tools;Diseases;Metadata;big data analytics;deep learning;disease detection},
  doi={10.1109/BigData.2018.8622504}}

@INPROCEEDINGS{9581175,
  author={Villanueva-Vega, Danny and Rodriguez-Martinez, Manuel},
  booktitle={2021 IEEE International Conference on Digital Health (ICDH)}, 
  title={Finding Similar Tweets in Health Related Topics}, 
  year={2021},
  volume={},
  number={},
  pages={184-190},
  keywords={Training;Social networking (online);Computational modeling;Data models;Real-time systems;Classification algorithms;Electronic healthcare;Deep Learning;tweets;similarity},
  doi={10.1109/ICDH52753.2021.00033}}

@article{9906925,
  author={Yilmaz, Tolga and Ulusoy, {\"O}zg{\"u}r},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Misinformation Propagation in Online Social Networks: Game Theoretic and Reinforcement Learning Approaches}, 
  year={2023},
  volume={10},
  number={6},
  pages={3321-3332},
  keywords={Games;Fake news;Social networking (online);Costs;Reinforcement learning;Receivers;Mathematical models;Cooperative systems;Cooperative games;misinformation propagation;online social networks (OSNs);reinforcement learning (RL)},
  doi={10.1109/TCSS.2022.3208793}}

@INPROCEEDINGS{10100054,
  author={Harbola, Aditya and Manchanda, Mahesh and Negi, Deepti},
  booktitle={2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA)}, 
  title={Misinformation classification using LSTM and BERT model}, 
  year={2023},
  volume={},
  number={},
  pages={1073-1077},
  keywords={Deep learning;Computational modeling;Bit error rate;Knowledge discovery;Information age;Natural language processing;Data models;Misinformation;Machine learning;LSTM;BERT},
  doi={10.1109/ICIDCA56705.2023.10100054}}

@article{article,
author = {T, Sadiq and Mathew, Saji},
year = {2022},
month = {05},
pages = {1-15},
title = {The disaster of misinformation: a review of research in social media},
volume = {13},
journal = {International Journal of Data Science and Analytics},
doi = {10.1007/s41060-022-00311-6}
}

@inproceedings{Ayoobi_2023, series={HT ’23},
   title={The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention},
   url={http://dx.doi.org/10.1145/3603163.3609064},
   DOI={10.1145/3603163.3609064},
   booktitle={Proceedings of the 34th ACM Conference on Hypertext and Social Media},
   publisher={ACM},
   author={Ayoobi, Navid and Shahriar, Sadat and Mukherjee, Arjun},
   year={2023},
   month=sep, collection={HT ’23} }

@article{bioinformatics,
    author = {Comeau, Donald C and Wei, Chih-Hsuan and Islamaj Doğan, Rezarta and Lu, Zhiyong},
    title = "{PMC text mining subset in BioC: about three million full-text articles and growing}",
    journal = {Bioinformatics},
    volume = {35},
    number = {18},
    pages = {3533-3535},
    year = {2019},
    month = {01},
    abstract = "{Interest in text mining full-text biomedical research articles is growing. To facilitate automated processing of nearly 3 million full-text articles (in PubMed Central® Open Access and Author Manuscript subsets) and to improve interoperability, we convert these articles to BioC, a community-driven simple data structure in either XML or JavaScript Object Notation format for conveniently sharing text and annotations.The resultant articles can be downloaded via both File Transfer Protocol for bulk access and a Web API for updates or a more focused collection. Since the availability of the Web API in 2017, our BioC collection has been widely used by the research community.https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz070},
    url = {https://doi.org/10.1093/bioinformatics/btz070},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/35/18/3533/48975610/bioinformatics\_35\_18\_3533.pdf},
}

@misc{chung2022scalinginstructionfinetunedlanguagemodels,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.11416}, 
}

@article{DBLP:journals/corr/abs-1810-04805,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2005-14165,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@Article{encyclopedia3040099,
AUTHOR = {Benaissa Pedriza, Samia},
TITLE = {Disinformation Perception by Digital and Social Audiences: Threat Awareness, Decision-Making and Trust in Media Organizations},
JOURNAL = {Encyclopedia},
VOLUME = {3},
YEAR = {2023},
NUMBER = {4},
PAGES = {1387--1400},
URL = {https://www.mdpi.com/2673-8392/3/4/99},
ISSN = {2673-8392},
ABSTRACT = {The effects of disinformation in the media and social networks have been extensively studied from the perspective of reception studies. However, the perception of this media phenomenon expressed by different types of audiences in distant geographic locations and with different media cultures has hardly been addressed by experts. This theoretical review study aims to analyze the relationship between the actual level of disinformation and the perception expressed by the audiences themselves. The results of the study reveal, firstly, that users of social networks and digital media do not perceive being surrounded by an excessively worrying volume of disinformation, a fact that contrasts with the data recorded, which are visibly higher. This situation reveals that the audience tends to normalize disinformation, which is intensively consumed on a daily basis and does not seem to worry the public in general terms, although some differences can be detected depending on variables such as gender, age or education. On the other hand, paradoxically, audiences visibly express rejection attitudes towards the channels that disseminate false information, with media outlets being the least trusted, despite recognizing that social networks are the place where more disinformation is generated and circulated at the same time.},
DOI = {10.3390/encyclopedia3040099}
}



@inproceedings{inproceedings,
author = {Deng, Xiang and Bashlovkina, Vasilisa and Han, Feng and Baumgartner, Simon and Bendersky, Michael},
year = {2023},
month = {04},
pages = {1014-1019},
title = {LLMs to the Moon? Reddit Market Sentiment Analysis with Large Language Models},
doi = {10.1145/3543873.3587605}
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{koh2023groundinglanguagemodelsimages,
      title={Grounding Language Models to Images for Multimodal Inputs and Outputs}, 
      author={Jing Yu Koh and Ruslan Salakhutdinov and Daniel Fried},
      year={2023},
      eprint={2301.13823},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.13823}, 
}

@misc{lewis2019bartdenoisingsequencetosequencepretraining,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.13461}, 
}

@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@misc{naveed2024comprehensiveoverviewlargelanguage,
      title={A Comprehensive Overview of Large Language Models}, 
      author={Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
      year={2024},
      eprint={2307.06435},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.06435}, 
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}


