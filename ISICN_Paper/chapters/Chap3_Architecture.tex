
\section{System Architecture}

\subsection{ETL Pipeline for Biomedical Papers}

Our model must use credible sources of information to rebut misinformation. We identified NIH PubMed \cite{pubmed}, an online library that contains peer-reviewed medical literature. We worked to extract relevant papers and store them in a vector database. To extract these papers, we used the BioC API \cite{bioinformatics}, which has access to the PubMed library. The API needs the research paper's identifier, known as PubMed Central (PMC) ID. We designed a web scraper to get these identifiers. The pipeline in Figure \ref{fig:etl} shows the processes of data extraction. 

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=0.75\textwidth]{figures/ETL_Pipeline.jpeg}
	\end{center}
	\caption{Medical Data Extraction Pipeline} 
	\label{fig:etl}
\end{figure}

\subsubsection{Scraper}
The first step of the pipeline was identifying what papers we needed to extract. We selected topics based on our datasets: \textit{allergy}, \textit{covid}, \textit{monkeypox}, \textit{zika}, \textit{vaccine}, and others. We built our scraper using Selenium and BeautifulSoup libraries to extract the links for each paper, and get the PMC ID  from the link.  We extracted 5000 PMC IDs for each topic.

\subsubsection{BioC API}
Using the PubMed API, BioC, we made requests with the PMC ID that returned each document as JSON text. Later, the paper's sections -introduction, methodology, results, and others- were extracted, excluding references. We removed tables, figures, and references from the context to ensure the chunking process worked appropriately. After that, we stored the result into a new JSON that contains the paper's metadata and context. 

\subsubsection{Vectorizing data}
Next, we vectorized the papers, as shown in Figure \ref{fig:vector}. First, each paper’s context was split into chunks using LangChain. Then, we used an LLM, BAAI \cite{bge_embedding}, to embed these chunks. A universal unique identifier (UUID) is combined with each chunk and stored in a Chroma database. After that, we added these UUIDs to their JSON. 

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{figures/Data_vectorization.jpeg} 
	\end{center}
	\caption{Data Vectorization Process} 
	\label{fig:vector}
\end{figure}


\subsubsection{Metadata Store}
Duplicates or any research that did not contain at least an abstract were removed. That ensures that there is no repetition or inconsistency when doing the rebuttal. Next, we upload the metadata into a Postgres database using the following schema:
\begin{description}
	\item{\textbf{Research:}}  Contains the research paper's metadata. Its attributes are: \textit{title} - paper's title; \textit{context} - the paper’s text; \textit{paper\_ref} - the reference of the paper; and \textit{fullpaper} - a boolean that is false if the paper only contains an abstract.
	\item{\textbf{Chunks:}} Pairs the UUIDs from the paper's chunks and their respective research record.  
	\item{\textbf{Keyword:}} Keywords that allow the reader to know the subjects mentioned in the paper. 
	\item{\textbf{Author:}} Full name of the paper's authors. 
	\item{\textbf{Reference:}} All references present in the paper.
	\item{\textbf{Topic:}} The topics used to search the papers.

\end{description}

We started the search with 85,000 peer-reviewed papers. After finishing the filtering and data cleaning, we ended with 56,365 different peer-reviewed papers. 


\subsection{Misinformation Rebuttal Pipeline (MRP)}
After training the models and storing the context for the rebuttal, we create the model pipeline. The pipeline shown in Figure \ref{fig:llm} shows the process of receiving a text, making the classifications, and returning an explanation of why it is misinformation.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=0.75\textwidth]{figures/LLM_Pipeline.jpeg} 
	\end{center}
	\caption{Misinformation Rebuttal Pipeline}
	\label{fig:llm}
\end{figure}


\subsubsection{Health-related Classification}
The first part of the pipeline is determining if the text is related to health. If the text is related, we go to the next part of the pipeline. Non-related or ambiguous texts end the process because it is out of the model's scope. 

\subsubsection{Misinformation Classification}
Next, we validate if the input contains misinformation. The model  only returns if the text misinformation or not. If the model finds no misinformation, the process is completed. When the result contains misinformation, we start the search for context for the rebuttal.

\subsubsection{Context Finder}
Before we  query the vector database, we must understand the topic of the text. To automate this process and generate a query as precise as possible, we use Ollama \cite{ollama} which runs the LLama 3.1 LLM. We send a query to Ollama asking it to make a one-sentence query for the vector database related to the input tweet.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{figures/Context_finder_process.png}
	\end{center}
	\caption{Context Finder Example} 
	\label{fig:cfe}
\end{figure}

\indent Figure \ref{fig:cfe} shows that Ollama can identify the topics of the original text. That output is sent to the Chroma database to retrieve the research papers' chunks. In our current setup, our model returns eight chunks. We selected
this number because it returns an appropriate amount of context without Ollama truncating the text. These chunks are then sent to another model to be analyzed and organized.

\subsubsection{Organize and Rebut}
The final part of our pipeline is using RAG to provide an answer that explains why the original text is misinformation. First, we retrieve the references of the chunks we use for the context. Then, we send the original tweet text, and the retrieved chunks  as context to Ollama so the model can generate an explanation that rebuts the misinformation. The final output is a JSON with the classifications, a 2-3 sentence rebuttal generated by Ollama, and references used for the rebuttal.

The pipeline automates the classification process and rebuts misinformation using peer-reviewed research. By leveraging fine-tuned models, vector search, and RAG, the architecture provides concise, fact-based responses. Also, this
approach has the ability to explain complex content in a manner accessible to non-technical readers. 


\subsection{User Interface (UI)}
We built a UI where users  can interact with tweets and see the classification/rebuttal. When we select a tweet, a screen appears showing the original text, the classifications, and rebuttal; this is shown in Figure \ref{fig:frontendrebuttal}. The top of the screen shows the input text, number 1 shows the health classification, and number 2 shows the misinformation classification. Below that is the rebuttal and references generated by the model. 
\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{figures/THS_Rebuttal_view.jpeg} 
	\end{center}
	\caption{THS Frontend  - Rebuttal View} 
	\label{fig:frontendrebuttal}
\end{figure}






